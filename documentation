Title: College Cost, Career Gain?

Abstract: The overall goal of our project is to examine the U.S. Department of Education data to examine college data and determine how elitism influences the earnings of students post-grad. While defining elitism is hard - many people have different definitions of what makes a school elite - we will be basing it off the assumption that the more money that students invest in their education has a direct relationship with how they view the value of their school of choice. 


Documentation:

Introduction: Summarize your project report in several paragraphs.
For this project, we aim to examine the relationship between college costs and post-grad earnings using the U.S. Dept. of Education’s College Scorecard. Our research question that we hope to answer is: Are post-graduate earnings truly driven by institutional prestige and cost, or are we overvaluing reputation? As college tuition continues to rise, students and family often assume that investing more in prestigious institutions guarantees higher earnings. This assumption not only impacts the decisions of students but also broader societal attitudes about the value of elite education. 

The implications of this problem challenge the connection between institution reputation and future success which calls attention to whether the higher cost of attending elite colleges is justified by potential outcomes. The pressure to be successful is high for students, especially those burdened by debt. It also has an impact on employers who may unknowingly perpetuate bias in hiring practices. If reputation is overvalued, it means that qualified graduates are potentially being overlooked if they graduated from less expensive schools who offer equal or greater potential. 

To address this problem, we used a multi-model approach using the College Scorecard data. Each team member applied a different modeling strategy to determine what drives post-grad earnings. Sydney used KNN to find earnings trends among similar institutions. Deborah applied Random Forests to identify the most important features and RMSE based on selected variables. Mark used LDA/QDA to explore any differences in variables. Nyla built Decision Trees to determine assumptions that can be made based on the available decision paths. 

By using different models to analyze the data, we were able to gain a holistic view. Previous research such as the National Bureau of Economic Research and the Foundation for Research on Equal Opportunity, often relies on ROI and regression based predictions. By focusing on tuition as a proxy for perceived prestige and comparing performance across different models, we were able to add value to previous work. This allowed us to test whether cost and school type (private, public) alone are reliable predictors of earning, or if we should focus on other factors. 

The key variables that we use are tuition, student demographics, completion rates, program type, median earnings, etc. Our analysis found that out-of-state tuition is moderately associated with higher post-grad earnings, and certain majors like Business and STEM have a stronger correlation with higher earnings. However, institutional features only provide moderate predictions for earnings which suggest that higher tuition and prestige alone don’t guarantee higher earnings. The limitations of the approach is that the models' performances suggest that there is a lot of unexplained variation in earrings. This emphasizes the need to explore factors beyond just tuition and the other variables that we explored. 

Setup: Set up the stage for your experimental results. 
The dataset that we used for this project is from the U.S. Department of Education College Scorecard, which includes detailed information on over 6,000 colleges and universities in the U.S.. It provides key features such as: Tuition cost, student loan debt, admission rates, enrollment size, location, highest degree awarded, race, first-generation status, graduation rates, withdrawal rates, and median earnings at 1, 5, and 10 years after graduation. Some variables in the dataset have missing data due to incomplete records for some schools or inconsistent reporting. To prepare the data for exploration, we performed data cleaning to get rid of missing values that could mess up our models. 

We ran exploratory analyses and classification models including KNN, decision trees, random forest, and LDA/QDA. For these models we used a variety of predictors to forecast 10 year postgraduate earnings as our outcome variable. For the KNN model, we set k=3 based on RMSE and R² to maximize the performance of the model while also ensuring that variance and bias is balanced. For the decision tree and random forest we used max_depth statements to ensure that our results are clear and readable. We executed these models in VS code integrating Python 3.9.13 via Anaconda using the following key packages: pandas, numpy, matplotlib, statsmodels, and key scikit-learn modules (e.g., KNeighborsRegressor, Pipeline, PCA, KMeans, GridSearchCV) while computation was performed on a local CPU.

This is a supervised regression task where the response variable is continuous (earnings). We evaluate the predictive performance of multiple models. For KNN we use a range of k values (1 to 20) and use forward/backwards selection to refine the optimal number of variables included in the model. For decision tree regressor, we tune max_depth, min_samples_split, and min_samples_leaf to balance bias-variance tradeoff. For random forest regressor, we vary n_estimators, max_depth, and max_features using GridSearchCV to find the best-performing ensemble configuration. We explore LDA and QDA, to classify earnings into discrete categories such as low, medium, and high earners to assess categorical prediction ability. Each model is trained and evaluated using train-test splits (80/20), and performance is measured via RMSE and R². For the classification approach, we also consider accuracy and confusion matrices.

Results: Describe the results from your experiments.
Main results: Describe the main experimental results you have; this is where you highlight the most interesting findings.
Supplementary results: Describe the parameter choices you have made while running the experiments. This part goes into justifying those choices.

Based on KNN analysis, we found that student demographics (race, first-generation) and student outcomes (completion rate, withdrawal rate) best predict post graduate earnings compared to institution information (geographic location, highest/most prominent degree awarded) or cost (in-state tuition, out-of-state tuition). These variables were favored in forward selection, reducing RMSE from 6753.68 to 5732.61. This suggests individual level factors that are not captured at the institutional level more likely drive earning potential in more direct ways. Future directions could explore such variables further including chosen major or family background. Furthermore, we explored if error was correlated with specific profiles, suggesting data bias or room for model improvement, but there was no significant signal between error and any of the included predictors.

The random forest model was trained to predict median postgraduate earnings 10 years after being in school using institutional-level variables such as tuition, enrollment size, and admission rate. The model achieved a mean squared error (MSE) of $90, 308, 508, which corresponds to a root mean squared error (RMSE) of $9,503. While these numbers sound very large, it is somewhat reasonable and explainable as we are working with real-life income that largely varies between individuals, and can often vary from tens to hundreds of thousands of dollars. Although the MSE was very large, the model’s R-squared value was 0.6433, which shows that 64% of the variance in postgraduate earnings can be explained by the model. This R-squared value also suggests a moderate performance to the dataset and is a fairly strong result given the limited set of features. 

The feature importance analysis revealed that out-of-state tuition (TUITIONFEE_OUT) was the most influential predictor, accounting for about 49% of the model’s decision-making weight. Admission rate (ADM_RATE) followed with 35% while undergraduate population size (UGDS) and in-state tuition (TUITIONFEE_IN) both had around 1% of weight. Finally, the region (REGION) and degree type variables (SCH_DEG) had almost negligible impact on earnings predictions. 
	
The random forest model implemented 100 estimators and used default parameters for maximum depth and feature selection – no significant performance improvements were sufficed when increasing the number of estimators past 100, so we just kept the estimator to be 100. Additionally, the train-test split was 80/20 to ensure sufficient training data while preserving a representative test set. Feature selection was also limited to just six variables in order to maintain interpretability and to avoid multicollinearity. And finally, median earnings with N/A or ‘PrivacySuppressed’ values were cleaned and excluded in order to ensure numerical consistency across training samples. 

For the decision tree model, we were able to split up the tree based on major type. From this we see that: Schools with more English and History majors and higher tuition can earn decent wages but there is high variability;Education majors are more likely to attend in-state/public schools. There is a lot of variability in pay for education majors;STEM students who attend higher tuition schools often have higher earnings;Business majors tend to see higher earnings especially when tuition is higher; For trade majors, Lower tuition fees tend to lead to lower earnings. As tuition increases above $27k for out of state students, earnings increase
In state tuition led to lower predicted earnings compared to out of state tuition.



Discussion: Discuss the results obtained above. If your results are very good, see if you could compare them with some existing approaches that you could find online. If your results are not as good as you had hoped for, make a good-faith diagnosis about what the problem is.

The KNN model demonstrated that institutional features are moderate predictors of postgraduate earnings. While the model showed some ability to generalize to the test set, there still remains room for further improvement. A key limitation stems from the dataset: due to a high volume of missing data in the original College Scorecard CSV, we had to perform data cleaning and restricted our analysis to the three most populated schools in each U.S. state and D.C. This prioritized data quality by minimizing the need for imputation as we focused on well documented observations. However, it reduced our sample size to just 153 institutions which we used to predict national patterns and trends. This limited sample not only constrains generalizability but may also introduce bias, as more populated institutions are often better funded, more renowned, and associated with stronger student outcomes. Thus, expanding the dataset to include a more representative set of institutions with well-documented features would likely improve model performance and reduce error.

Overall, the random forest model produced moderate results. The moderate R-squared value suggests that tuition levels and institutional selectivity can carry meaningful predictive power in explaining variation in postgraduate earnings. However, the high MSE value indicates that while this model predicts general trends, it lacks the precision needed for specific and individual predictions. This could be due to the limited feature set, which doesn’t include other factors that are known to heavily influence earnings such as major, work experience, and demographic background.

The decision tree model shows that a student’s college major plays a huge role in potential post grad earnings, with tuition costs and in/out of state status having major influence. Humanities and education majors have high variability in earnings with education majors other attending in state schools where pay caps limit income. Stem and business majors tend to see higher earnings when students attend higher tuition schools. Trade majors show a threshold effect where earnings increase once tuition is more than $27,000 which suggests that more expensive program lead to better economic outcomes. Overall we see that out of state tuition is associated with higher earnings which is interesting as most out of state schools are private.

Conclusion: In several sentences, summarize what you have done in this project.

In this project, we used data from the U.S. Department of Education’s College Scorecard to predict post-graduation median earnings based on institutional features such as tuition, admission rate, completion rate, and student demographics. We implemented and evaluated several models, including KNN, Decision Trees, Random Forests, and LDA/QDA. Model performance was assessed using metrics such as RMSE and R², and we explored the impact of model choice, hyperparameter tuning, and feature selection on predictive accuracy. Our findings suggest that while institutional features offer some predictive power, expanding beyond institutional level data to individual factors as well as refining the dataset would likely yield stronger and more generalizable results.

References: Put any links, papers, blog posts, or GitHub repositories that you have borrowed from/found useful here.
https://collegescorecard.ed.gov/data
https://www.goacta.org/2020/07/how-elitism-affects-higher-education/
https://www.nber.org/digest/dec99/payoff-attending-elite-college
https://www.hceducationconsulting.com/2023/09/13/unlocking-prosperity-the-higher-earning-potential-of-elite-college-graduates/
